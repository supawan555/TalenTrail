{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bb33185",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bdba0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_resume = pd.read_csv(\"data/resume.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a0af1ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Data Science', 'HR', 'Advocate', 'Arts', 'Web Designing',\n",
       "       'Mechanical Engineer', 'Sales', 'Health and fitness',\n",
       "       'Civil Engineer', 'Java Developer', 'Business Analyst',\n",
       "       'SAP Developer', 'Automation Testing', 'Electrical Engineering',\n",
       "       'Operations Manager', 'Python Developer', 'DevOps Engineer',\n",
       "       'Network Security Engineer', 'PMO', 'Database', 'Hadoop',\n",
       "       'ETL Developer', 'DotNet Developer', 'Blockchain', 'Testing'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_resume.Category.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b97ab431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(962, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_resume.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da26cb89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_resume = df_resume.reindex(np.random.permutation(df_resume.index))\n",
    "df_resume = df_resume.copy().iloc[:500, :]\n",
    "df_resume.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77447168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Skills * Programming Languages: Python (pandas, numpy, scipy, scikit-learn, matplotlib), Sql, Java, JavaScript/JQuery. * Machine learning: Regression, SVM, NaÃ¯ve Bayes, KNN, Random Forest, Decision Trees, Boosting techniques, Cluster Analysis, Word Embedding, Sentiment Analysis, Natural Language processing, Dimensionality reduction, Topic Modelling (LDA, NMF), PCA & Neural Nets. * Database Visualizations: Mysql, SqlServer, Cassandra, Hbase, ElasticSearch D3.js, DC.js, Plotly, kibana, matplotlib, ggplot, Tableau. * Others: Regular Expression, HTML, CSS, Angular 6, Logstash, Kafka, Python Flask, Git, Docker, computer vision - Open CV and understanding of Deep learning.Education Details \\r\\n\\r\\nData Science Assurance Associate \\r\\n\\r\\nData Science Assurance Associate - Ernst & Young LLP\\r\\nSkill Details \\r\\nJAVASCRIPT- Exprience - 24 months\\r\\njQuery- Exprience - 24 months\\r\\nPython- Exprience - 24 monthsCompany Details \\r\\ncompany - Ernst & Young LLP\\r\\ndescription - Fraud Investigations and Dispute Services   Assurance\\r\\nTECHNOLOGY ASSISTED REVIEW\\r\\nTAR (Technology Assisted Review) assists in accelerating the review process and run analytics and generate reports.\\r\\n* Core member of a team helped in developing automated review platform tool from scratch for assisting E discovery domain, this tool implements predictive coding and topic modelling by automating reviews, resulting in reduced labor costs and time spent during the lawyers review.\\r\\n* Understand the end to end flow of the solution, doing research and development for classification models, predictive analysis and mining of the information present in text data. Worked on analyzing the outputs and precision monitoring for the entire tool.\\r\\n* TAR assists in predictive coding, topic modelling from the evidence by following EY standards. Developed the classifier models in order to identify \"red flags\" and fraud-related issues.\\r\\n\\r\\nTools & Technologies: Python, scikit-learn, tfidf, word2vec, doc2vec, cosine similarity, NaÃ¯ve Bayes, LDA, NMF for topic modelling, Vader and text blob for sentiment analysis. Matplot lib, Tableau dashboard for reporting.\\r\\n\\r\\nMULTIPLE DATA SCIENCE AND ANALYTIC PROJECTS (USA CLIENTS)\\r\\nTEXT ANALYTICS - MOTOR VEHICLE CUSTOMER REVIEW DATA * Received customer feedback survey data for past one year. Performed sentiment (Positive, Negative & Neutral) and time series analysis on customer comments across all 4 categories.\\r\\n* Created heat map of terms by survey category based on frequency of words * Extracted Positive and Negative words across all the Survey categories and plotted Word cloud.\\r\\n* Created customized tableau dashboards for effective reporting and visualizations.\\r\\nCHATBOT * Developed a user friendly chatbot for one of our Products which handle simple questions about hours of operation, reservation options and so on.\\r\\n* This chat bot serves entire product related questions. Giving overview of tool via QA platform and also give recommendation responses so that user question to build chain of relevant answer.\\r\\n* This too has intelligence to build the pipeline of questions as per user requirement and asks the relevant /recommended questions.\\r\\n\\r\\nTools & Technologies: Python, Natural language processing, NLTK, spacy, topic modelling, Sentiment analysis, Word Embedding, scikit-learn, JavaScript/JQuery, SqlServer\\r\\n\\r\\nINFORMATION GOVERNANCE\\r\\nOrganizations to make informed decisions about all of the information they store. The integrated Information Governance portfolio synthesizes intelligence across unstructured data sources and facilitates action to ensure organizations are best positioned to counter information risk.\\r\\n* Scan data from multiple sources of formats and parse different file formats, extract Meta data information, push results for indexing elastic search and created customized, interactive dashboards using kibana.\\r\\n* Preforming ROT Analysis on the data which give information of data which helps identify content that is either Redundant, Outdated, or Trivial.\\r\\n* Preforming full-text search analysis on elastic search with predefined methods which can tag as (PII) personally identifiable information (social security numbers, addresses, names, etc.) which frequently targeted during cyber-attacks.\\r\\nTools & Technologies: Python, Flask, Elastic Search, Kibana\\r\\n\\r\\nFRAUD ANALYTIC PLATFORM\\r\\nFraud Analytics and investigative platform to review all red flag cases.\\r\\nâ\\x80¢ FAP is a Fraud Analytics and investigative platform with inbuilt case manager and suite of Analytics for various ERP systems.\\r\\n* It can be used by clients to interrogate their Accounting systems for identifying the anomalies which can be indicators of fraud by running advanced analytics\\r\\nTools & Technologies: HTML, JavaScript, SqlServer, JQuery, CSS, Bootstrap, Node.js, D3.js, DC.js'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_resume['Resume'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9740c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import string\n",
    "def clear_fun(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', ' ', text)\n",
    "    text = re.sub(\"\\\\W\",\" \",text) \n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', ' ', text)\n",
    "    text = re.sub('<.*?>+', ' ', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    text = re.sub('\\w*\\d\\w*', ' ', text) \n",
    "   # text = re.sub('\\s+', '', text)\n",
    "    return text\n",
    "\n",
    "df_resume['Resume'] = df_resume['Resume'].apply(clear_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b1c6932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'skills   programming languages  python  pandas  numpy  scipy  scikit learn  matplotlib   sql  java  javascript jquery    machine learning  regression  svm  naã ve bayes  knn  random forest  decision trees  boosting techniques  cluster analysis  word embedding  sentiment analysis  natural language processing  dimensionality reduction  topic modelling  lda  nmf   pca   neural nets    database visualizations  mysql  sqlserver  cassandra  hbase  elasticsearch   js  dc js  plotly  kibana  matplotlib  ggplot  tableau    others  regular expression  html  css  angular    logstash  kafka  python flask  git  docker  computer vision   open cv and understanding of deep learning education details     data science assurance associate     data science assurance associate   ernst   young llp  skill details   javascript  exprience     months  jquery  exprience     months  python  exprience     monthscompany details   company   ernst   young llp  description   fraud investigations and dispute services   assurance  technology assisted review  tar  technology assisted review  assists in accelerating the review process and run analytics and generate reports     core member of a team helped in developing automated review platform tool from scratch for assisting e discovery domain  this tool implements predictive coding and topic modelling by automating reviews  resulting in reduced labor costs and time spent during the lawyers review     understand the end to end flow of the solution  doing research and development for classification models  predictive analysis and mining of the information present in text data  worked on analyzing the outputs and precision monitoring for the entire tool     tar assists in predictive coding  topic modelling from the evidence by following ey standards  developed the classifier models in order to identify  red flags  and fraud related issues     tools   technologies  python  scikit learn  tfidf        cosine similarity  naã ve bayes  lda  nmf for topic modelling  vader and text blob for sentiment analysis  matplot lib  tableau dashboard for reporting     multiple data science and analytic projects  usa clients   text analytics   motor vehicle customer review data   received customer feedback survey data for past one year  performed sentiment  positive  negative   neutral  and time series analysis on customer comments across all   categories     created heat map of terms by survey category based on frequency of words   extracted positive and negative words across all the survey categories and plotted word cloud     created customized tableau dashboards for effective reporting and visualizations   chatbot   developed a user friendly chatbot for one of our products which handle simple questions about hours of operation  reservation options and so on     this chat bot serves entire product related questions  giving overview of tool via qa platform and also give recommendation responses so that user question to build chain of relevant answer     this too has intelligence to build the pipeline of questions as per user requirement and asks the relevant  recommended questions     tools   technologies  python  natural language processing  nltk  spacy  topic modelling  sentiment analysis  word embedding  scikit learn  javascript jquery  sqlserver    information governance  organizations to make informed decisions about all of the information they store  the integrated information governance portfolio synthesizes intelligence across unstructured data sources and facilitates action to ensure organizations are best positioned to counter information risk     scan data from multiple sources of formats and parse different file formats  extract meta data information  push results for indexing elastic search and created customized  interactive dashboards using kibana     preforming rot analysis on the data which give information of data which helps identify content that is either redundant  outdated  or trivial     preforming full text search analysis on elastic search with predefined methods which can tag as  pii  personally identifiable information  social security numbers  addresses  names  etc   which frequently targeted during cyber attacks   tools   technologies  python  flask  elastic search  kibana    fraud analytic platform  fraud analytics and investigative platform to review all red flag cases   â   fap is a fraud analytics and investigative platform with inbuilt case manager and suite of analytics for various erp systems     it can be used by clients to interrogate their accounting systems for identifying the anomalies which can be indicators of fraud by running advanced analytics  tools   technologies  html  javascript  sqlserver  jquery  css  bootstrap  node js    js  dc js'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_resume['Resume'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70b78cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(df_resume['Category'])\n",
    "df_resume['Category'] = le.transform(df_resume['Category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83e7af72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 12,  3,  2,  8,  7, 15, 22,  0, 11,  5, 20, 14, 23, 16,  6, 18,\n",
       "       19, 24,  9, 21, 13,  1, 17,  4])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_resume['Category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "286cb841",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer (stop_words='english')\n",
    "\n",
    "tfidf.fit(df_resume['Resume'])\n",
    "requredTaxt = tfidf.transform(df_resume['Resume'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9dfd635b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
       "\twith 81273 stored elements and shape (500, 6750)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requredTaxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a9db8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(requredTaxt,df_resume['Category'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2bece7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 6750)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ddcc336d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 6750)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bbfc5032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "clf = OneVsRestClassifier(KNeighborsClassifier())\n",
    "clf.fit(X_train,y_train)\n",
    "ypred = clf.predict(X_test)\n",
    "print (accuracy_score(y_test,ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "042fc33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(tfidf,open('tfidf.pkl','wb'))\n",
    "pickle.dump(clf,open('clf.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "30e38242",
   "metadata": {},
   "outputs": [],
   "source": [
    "myresume = \"Built REST APIs with FastAPI, Docker, CI/CD (GitHub Actions), MongoDB/PostgreSQL, and React front-ends.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1dc01a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted Category: Blockchain\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "clf = pickle.load(open('clf.pkl','rb'))\n",
    "cleaned_resume = clear_fun(myresume)\n",
    "input_features = tfidf.transform([cleaned_resume])\n",
    "prediction_id = clf.predict(input_features)[0]\n",
    "category_mapping={\n",
    "    6:'Data Science',\n",
    "    12: 'HR',\n",
    "    0:'Advocate',\n",
    "    1:'Arts', \n",
    "    24:'Web Designing',\n",
    "    16:'Mechanical Engineer',\n",
    "    22:'Sales',\n",
    "    14:'Health and fitness',\n",
    "    5:'Civil Engineer',  \n",
    "    15:'Java Developer',\n",
    "    4:'Business Analyst',\n",
    "    21:'SAP Developer', \n",
    "    2:'Automation Testing',\n",
    "    11:'Electrical Engineering',\n",
    "    18:'Operations Manager',\n",
    "    20:'Python Developer', \n",
    "    8:'DevOps Engineer',\n",
    "    17:'Network Security Engineer',\n",
    "    19:'PMO', \n",
    "    7:'Database',\n",
    "    13:'Hadoop',\n",
    "    10:'ETL Developer',\n",
    "    9:'DotNet Developer', \n",
    "    3:'Blockchain',\n",
    "    23:'Testing',\n",
    "    \n",
    "}\n",
    "category_name = category_mapping.get(prediction_id,'Unknown')\n",
    "print('predicted Category:', category_name)\n",
    "print(prediction_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
